{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shinkansen Passenger Satisfaction — V2\n",
    "\n",
    "**Goal:** Beat 0.9584855 accuracy (1st place). Current best: 0.9555643 (3rd place).\n",
    "\n",
    "**Strategy:** Optuna-tuned LightGBM + XGBoost + CatBoost → stacking ensemble → threshold optimization → seed averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU enabled: True\n",
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Configuration\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = '.'\n",
    "\n",
    "# --- GPU Configuration ---\n",
    "# Set USE_GPU = False if you don't have a CUDA-capable GPU.\n",
    "# LightGBM stays on CPU (pip build lacks CUDA support).\n",
    "# XGBoost and CatBoost use GPU when available.\n",
    "USE_GPU = True\n",
    "\n",
    "XGB_DEVICE = {'device': 'cuda'} if USE_GPU else {}\n",
    "CB_DEVICE = {'task_type': 'GPU'} if USE_GPU else {}\n",
    "\n",
    "print(f'GPU enabled: {USE_GPU}')\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (94379, 25), Test: (35602, 24)\n",
      "Target distribution: {1: 0.5466576251072802, 0: 0.4533423748927198}\n",
      "\n",
      "Missing values (train):\n",
      "Type_Travel                9226\n",
      "Customer_Type              8951\n",
      "Arrival_Time_Convenient    8930\n",
      "Catering                   8741\n",
      "Onboard_Service            7601\n",
      "Arrival_Delay_in_Mins       357\n",
      "Baggage_Handling            142\n",
      "Online_Support               91\n",
      "Legroom                      90\n",
      "Gender                       77\n",
      "CheckIn_Service              77\n",
      "Ease_of_Online_Booking       73\n",
      "Seat_Comfort                 61\n",
      "Departure_Delay_in_Mins      57\n",
      "Age                          33\n",
      "Platform_Location            30\n",
      "Onboard_Wifi_Service         30\n",
      "Onboard_Entertainment        18\n",
      "Cleanliness                   6\n",
      "Online_Boarding               6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading & Merging\n",
    "\n",
    "travel_train = pd.read_csv(f'{DATA_DIR}/Traveldata_train_(1).csv')\n",
    "survey_train = pd.read_csv(f'{DATA_DIR}/Surveydata_train_(1).csv')\n",
    "travel_test = pd.read_csv(f'{DATA_DIR}/Traveldata_test_(1).csv')\n",
    "survey_test = pd.read_csv(f'{DATA_DIR}/Surveydata_test_(1).csv')\n",
    "\n",
    "train = travel_train.merge(survey_train, on='ID', how='inner')\n",
    "test = travel_test.merge(survey_test, on='ID', how='inner')\n",
    "\n",
    "target_col = 'Overall_Experience'\n",
    "y_train = train[target_col].values\n",
    "test_ids = test['ID'].values\n",
    "\n",
    "print(f'Train: {train.shape}, Test: {test.shape}')\n",
    "print(f'Target distribution: {pd.Series(y_train).value_counts(normalize=True).to_dict()}')\n",
    "print(f'\\nMissing values (train):')\n",
    "missing = train.isnull().sum()\n",
    "print(missing[missing > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding done\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Encoding\n",
    "\n",
    "# Ordinal survey columns (same scale)\n",
    "survey_cols = [\n",
    "    'Seat_Comfort', 'Arrival_Time_Convenient', 'Catering',\n",
    "    'Onboard_Wifi_Service', 'Onboard_Entertainment', 'Online_Support',\n",
    "    'Ease_of_Online_Booking', 'Onboard_Service', 'Legroom',\n",
    "    'Baggage_Handling', 'CheckIn_Service', 'Cleanliness', 'Online_Boarding'\n",
    "]\n",
    "ordinal_map = {\n",
    "    'Extremely Poor': 0, 'Poor': 1, 'Needs Improvement': 2,\n",
    "    'Acceptable': 3, 'Good': 4, 'Excellent': 5\n",
    "}\n",
    "\n",
    "# Platform_Location has its own scale\n",
    "platform_map = {\n",
    "    'Very Inconvenient': 0, 'Inconvenient': 1, 'Needs Improvement': 2,\n",
    "    'Manageable': 3, 'Convenient': 4, 'Very Convenient': 5\n",
    "}\n",
    "\n",
    "# Nominal categoricals\n",
    "nominal_cols = ['Gender', 'Customer_Type', 'Type_Travel', 'Travel_Class', 'Seat_Class']\n",
    "\n",
    "def encode_data(df):\n",
    "    df = df.copy()\n",
    "    for col in survey_cols:\n",
    "        df[col] = df[col].map(ordinal_map)\n",
    "    df['Platform_Location'] = df['Platform_Location'].map(platform_map)\n",
    "    return df\n",
    "\n",
    "train_enc = encode_data(train)\n",
    "test_enc = encode_data(test)\n",
    "\n",
    "# Label encode nominal categoricals (fit on combined to handle unseen labels)\n",
    "label_encoders = {}\n",
    "for col in nominal_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_enc[col], test_enc[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_enc[col] = le.transform(train_enc[col].astype(str))\n",
    "    test_enc[col] = le.transform(test_enc[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Restore NaN where original was NaN (LabelEncoder converts 'nan' to an int)\n",
    "for col in nominal_cols:\n",
    "    train_enc.loc[train[col].isna(), col] = np.nan\n",
    "    test_enc.loc[test[col].isna(), col] = np.nan\n",
    "\n",
    "print('Encoding done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 46\n",
      "X_train: (94379, 46), X_test: (35602, 46)\n",
      "\n",
      "New feature columns:\n",
      "['survey_median', 'survey_skew', 'mid_ratings_count', 'delay_to_distance', 'age_bin', 'is_missing_Customer_Type', 'is_missing_Type_Travel', 'is_missing_Arrival_Time_Convenient', 'is_missing_Catering', 'is_missing_Onboard_Service', 'TypeTravel_x_TravelClass', 'CustType_x_SeatClass', 'survey_sum']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Enhanced Feature Engineering\n",
    "\n",
    "all_survey_cols = survey_cols + ['Platform_Location']  # all 14 ordinal columns\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    survey_data = df[all_survey_cols]\n",
    "    \n",
    "    # --- V1 features ---\n",
    "    df['survey_mean'] = survey_data.mean(axis=1)\n",
    "    df['survey_std'] = survey_data.std(axis=1)\n",
    "    df['survey_min'] = survey_data.min(axis=1)\n",
    "    df['survey_max'] = survey_data.max(axis=1)\n",
    "    df['survey_range'] = df['survey_max'] - df['survey_min']\n",
    "    df['high_ratings_count'] = (survey_data >= 4).sum(axis=1)\n",
    "    df['low_ratings_count'] = (survey_data <= 1).sum(axis=1)\n",
    "    df['total_delay'] = df['Departure_Delay_in_Mins'].fillna(0) + df['Arrival_Delay_in_Mins'].fillna(0)\n",
    "    df['delay_diff'] = df['Arrival_Delay_in_Mins'].fillna(0) - df['Departure_Delay_in_Mins'].fillna(0)\n",
    "    df['has_delay'] = ((df['Departure_Delay_in_Mins'].fillna(0) > 0) | (df['Arrival_Delay_in_Mins'].fillna(0) > 0)).astype(int)\n",
    "    \n",
    "    # --- V2 new features ---\n",
    "    df['survey_median'] = survey_data.median(axis=1)\n",
    "    df['survey_skew'] = survey_data.apply(lambda row: row.dropna().skew() if row.dropna().shape[0] >= 3 else 0, axis=1)\n",
    "    df['mid_ratings_count'] = ((survey_data == 2) | (survey_data == 3)).sum(axis=1)\n",
    "    \n",
    "    # Delay-to-distance ratio\n",
    "    df['delay_to_distance'] = df['total_delay'] / (df['Travel_Distance'] + 1)\n",
    "    \n",
    "    # Age quantile bins\n",
    "    df['age_bin'] = pd.qcut(df['Age'], q=10, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Missing value indicators (for columns with >1% missing)\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().mean() > 0.01:\n",
    "            df[f'is_missing_{col}'] = df[col].isna().astype(int)\n",
    "    \n",
    "    # Interaction features\n",
    "    df['TypeTravel_x_TravelClass'] = df['Type_Travel'].fillna(-1).astype(int) * 10 + df['Travel_Class'].fillna(-1).astype(int)\n",
    "    df['CustType_x_SeatClass'] = df['Customer_Type'].fillna(-1).astype(int) * 10 + df['Seat_Class'].fillna(-1).astype(int)\n",
    "    \n",
    "    # Survey sum (total satisfaction score)\n",
    "    df['survey_sum'] = survey_data.sum(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_fe = engineer_features(train_enc)\n",
    "test_fe = engineer_features(test_enc)\n",
    "\n",
    "# Drop non-feature columns\n",
    "drop_cols = ['ID', target_col]\n",
    "feature_cols = [c for c in train_fe.columns if c not in drop_cols]\n",
    "\n",
    "X_train = train_fe[feature_cols].values.astype(np.float32)\n",
    "X_test = test_fe[feature_cols].values.astype(np.float32)\n",
    "\n",
    "print(f'Features: {len(feature_cols)}')\n",
    "print(f'X_train: {X_train.shape}, X_test: {X_test.shape}')\n",
    "print(f'\\nNew feature columns:')\n",
    "v2_features = ['survey_median', 'survey_skew', 'mid_ratings_count', 'delay_to_distance',\n",
    "               'age_bin', 'TypeTravel_x_TravelClass', 'CustType_x_SeatClass', 'survey_sum']\n",
    "print([c for c in feature_cols if any(c.startswith(v) for v in v2_features + ['is_missing_'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.953751\n",
      "Fold 2: 0.956082\n",
      "Fold 3: 0.957512\n",
      "Fold 4: 0.956453\n",
      "Fold 5: 0.955550\n",
      "\n",
      "V1 Baseline CV: 0.955869 ± 0.001239\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Baseline — Reproduce V1 LightGBM\n",
    "\n",
    "v1_params = {\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.03,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 63,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_samples': 30,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'random_state': SEED,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "baseline_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**v1_params)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    preds = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    baseline_scores.append(acc)\n",
    "    print(f'Fold {fold+1}: {acc:.6f}')\n",
    "\n",
    "print(f'\\nV1 Baseline CV: {np.mean(baseline_scores):.6f} ± {np.std(baseline_scores):.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 52. Best value: 0.957385: 100%|██████████| 100/100 [55:24<00:00, 33.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM CV: 0.957385\n",
      "Best params: {'learning_rate': 0.02006130404098335, 'max_depth': 10, 'num_leaves': 82, 'subsample': 0.6363020221407356, 'colsample_bytree': 0.6163799680541543, 'min_child_samples': 14, 'reg_alpha': 0.4959859942324878, 'reg_lambda': 0.028694549312225034}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Optuna Hyperparameter Tuning — LightGBM\n",
    "\n",
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 127),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'random_state': SEED,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='maximize', study_name='lgb')\n",
    "study_lgb.optimize(lgb_objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest LightGBM CV: {study_lgb.best_value:.6f}')\n",
    "print(f'Best params: {study_lgb.best_params}')\n",
    "\n",
    "best_lgb_params = study_lgb.best_params\n",
    "best_lgb_params.update({'n_estimators': 5000, 'random_state': SEED, 'verbose': -1, 'n_jobs': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 81. Best value: 0.957321: 100%|██████████| 100/100 [59:09<00:00, 35.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best XGBoost CV: 0.957321\n",
      "Best params: {'learning_rate': 0.02629765461385586, 'max_depth': 10, 'subsample': 0.933787944562583, 'colsample_bytree': 0.8418629840543196, 'min_child_weight': 2, 'reg_alpha': 0.015500510498064968, 'reg_lambda': 0.0024044203758951436, 'gamma': 0.38915388203943474}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Optuna Hyperparameter Tuning — XGBoost (GPU-accelerated)\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'random_state': SEED,\n",
    "        'eval_metric': 'error',\n",
    "        'verbosity': 0,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist',\n",
    "        **XGB_DEVICE,  # GPU: adds device='cuda' when USE_GPU=True\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name='xgb')\n",
    "study_xgb.optimize(xgb_objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest XGBoost CV: {study_xgb.best_value:.6f}')\n",
    "print(f'Best params: {study_xgb.best_params}')\n",
    "\n",
    "best_xgb_params = study_xgb.best_params\n",
    "best_xgb_params.update({'n_estimators': 5000, 'random_state': SEED, 'eval_metric': 'error', 'verbosity': 0, 'n_jobs': -1, 'tree_method': 'hist', **XGB_DEVICE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 31. Best value: 0.956993: 100%|██████████| 75/75 [26:24<00:00, 21.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best CatBoost CV: 0.956993\n",
      "Best params: {'learning_rate': 0.06037233501239291, 'depth': 8, 'l2_leaf_reg': 0.24566928171583854, 'border_count': 68, 'bagging_temperature': 0.3576537029657124, 'random_strength': 0.14575253908437555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Optuna Hyperparameter Tuning — CatBoost (GPU-accelerated)\n",
    "\n",
    "def cb_objective(trial):\n",
    "    params = {\n",
    "        'iterations': 5000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 5.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 0.1, 10.0, log=True),\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'allow_writing_files': False,\n",
    "        'eval_metric': 'Accuracy',\n",
    "        'early_stopping_rounds': 100,\n",
    "        **CB_DEVICE,  # GPU: adds task_type='GPU' when USE_GPU=True\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=(X_val, y_val),\n",
    "            verbose=0\n",
    "        )\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, preds))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "study_cb = optuna.create_study(direction='maximize', study_name='catboost')\n",
    "study_cb.optimize(cb_objective, n_trials=75, show_progress_bar=True)\n",
    "\n",
    "print(f'\\nBest CatBoost CV: {study_cb.best_value:.6f}')\n",
    "print(f'Best params: {study_cb.best_params}')\n",
    "\n",
    "best_cb_params = study_cb.best_params\n",
    "best_cb_params.update({'iterations': 5000, 'random_seed': SEED, 'verbose': 0, 'allow_writing_files': False, 'eval_metric': 'Accuracy', 'early_stopping_rounds': 100, **CB_DEVICE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM OOF:\n",
      "  Fold 1: 0.954651\n",
      "  Fold 2: 0.957618\n",
      "  Fold 3: 0.959260\n",
      "  Fold 4: 0.958095\n",
      "  Fold 5: 0.957298\n",
      "  LGB OOF accuracy: 0.957385\n",
      "\n",
      "XGBoost OOF:\n",
      "  Fold 1: 0.955181\n",
      "  Fold 2: 0.957671\n",
      "  Fold 3: 0.959207\n",
      "  Fold 4: 0.957936\n",
      "  Fold 5: 0.956609\n",
      "  XGB OOF accuracy: 0.957321\n",
      "\n",
      "CatBoost OOF:\n",
      "  Fold 1: 0.954016\n",
      "  Fold 2: 0.957989\n",
      "  Fold 3: 0.958042\n",
      "  Fold 4: 0.958042\n",
      "  Fold 5: 0.956874\n",
      "  CB OOF accuracy: 0.956993\n",
      "\n",
      "Stacked ensemble OOF accuracy: 0.957353\n",
      "Meta-learner coefficients: [2.8281161  1.81549566 4.08693858]\n",
      "\n",
      "Best weighted average OOF accuracy: 0.957819\n",
      "Best weights (LGB, XGB, CB): (0.25, 0.40, 0.35)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Stacking Ensemble\n",
    "#\n",
    "# Generate out-of-fold predictions from each tuned model,\n",
    "# then train a logistic regression meta-learner on top.\n",
    "\n",
    "def get_oof_predictions(model_class, params, X, y, X_test, n_folds=N_FOLDS, fit_kwargs=None):\n",
    "    \"\"\"Returns (oof_probs, test_probs_avg) for a given model.\"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
    "    oof_probs = np.zeros(len(X))\n",
    "    test_probs = np.zeros(len(X_test))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        model = model_class(**params)\n",
    "        if fit_kwargs:\n",
    "            kwargs = {}\n",
    "            if 'lgb' in fit_kwargs:\n",
    "                kwargs = {\n",
    "                    'eval_set': [(X_val, y_val)],\n",
    "                    'callbacks': [lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)]\n",
    "                }\n",
    "            elif 'xgb' in fit_kwargs:\n",
    "                kwargs = {\n",
    "                    'eval_set': [(X_val, y_val)],\n",
    "                    'verbose': False\n",
    "                }\n",
    "            elif 'cb' in fit_kwargs:\n",
    "                kwargs = {\n",
    "                    'eval_set': (X_val, y_val),\n",
    "                    'verbose': 0\n",
    "                }\n",
    "            model.fit(X_tr, y_tr, **kwargs)\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        \n",
    "        oof_probs[val_idx] = model.predict_proba(X_val)[:, 1]\n",
    "        test_probs += model.predict_proba(X_test)[:, 1] / n_folds\n",
    "        \n",
    "        acc = accuracy_score(y_val, (oof_probs[val_idx] > 0.5).astype(int))\n",
    "        print(f'  Fold {fold+1}: {acc:.6f}')\n",
    "    \n",
    "    return oof_probs, test_probs\n",
    "\n",
    "# --- LightGBM OOF ---\n",
    "print('LightGBM OOF:')\n",
    "oof_lgb, test_lgb = get_oof_predictions(\n",
    "    lgb.LGBMClassifier, best_lgb_params, X_train, y_train, X_test, fit_kwargs='lgb'\n",
    ")\n",
    "print(f'  LGB OOF accuracy: {accuracy_score(y_train, (oof_lgb > 0.5).astype(int)):.6f}\\n')\n",
    "\n",
    "# --- XGBoost OOF ---\n",
    "print('XGBoost OOF:')\n",
    "oof_xgb, test_xgb = get_oof_predictions(\n",
    "    xgb.XGBClassifier, best_xgb_params, X_train, y_train, X_test, fit_kwargs='xgb'\n",
    ")\n",
    "print(f'  XGB OOF accuracy: {accuracy_score(y_train, (oof_xgb > 0.5).astype(int)):.6f}\\n')\n",
    "\n",
    "# --- CatBoost OOF ---\n",
    "print('CatBoost OOF:')\n",
    "oof_cb, test_cb = get_oof_predictions(\n",
    "    CatBoostClassifier, best_cb_params, X_train, y_train, X_test, fit_kwargs='cb'\n",
    ")\n",
    "print(f'  CB OOF accuracy: {accuracy_score(y_train, (oof_cb > 0.5).astype(int)):.6f}\\n')\n",
    "\n",
    "# --- Meta-learner: Logistic Regression ---\n",
    "oof_stack = np.column_stack([oof_lgb, oof_xgb, oof_cb])\n",
    "test_stack = np.column_stack([test_lgb, test_xgb, test_cb])\n",
    "\n",
    "meta = LogisticRegression(C=1.0, random_state=SEED, max_iter=1000)\n",
    "meta.fit(oof_stack, y_train)\n",
    "\n",
    "oof_meta_probs = meta.predict_proba(oof_stack)[:, 1]\n",
    "meta_acc = accuracy_score(y_train, meta.predict(oof_stack))\n",
    "print(f'Stacked ensemble OOF accuracy: {meta_acc:.6f}')\n",
    "print(f'Meta-learner coefficients: {meta.coef_[0]}')\n",
    "\n",
    "# --- Simple weighted average comparison ---\n",
    "# Optimize weights via grid search\n",
    "best_wavg_acc = 0\n",
    "best_weights = None\n",
    "for w1 in np.arange(0.1, 0.8, 0.05):\n",
    "    for w2 in np.arange(0.1, 0.8, 0.05):\n",
    "        w3 = 1 - w1 - w2\n",
    "        if w3 < 0.05:\n",
    "            continue\n",
    "        avg_probs = w1 * oof_lgb + w2 * oof_xgb + w3 * oof_cb\n",
    "        acc = accuracy_score(y_train, (avg_probs > 0.5).astype(int))\n",
    "        if acc > best_wavg_acc:\n",
    "            best_wavg_acc = acc\n",
    "            best_weights = (w1, w2, w3)\n",
    "\n",
    "print(f'\\nBest weighted average OOF accuracy: {best_wavg_acc:.6f}')\n",
    "print(f'Best weights (LGB, XGB, CB): ({best_weights[0]:.2f}, {best_weights[1]:.2f}, {best_weights[2]:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weighted average probabilities ((np.float64(0.25000000000000006), np.float64(0.40000000000000013), np.float64(0.34999999999999987)))\n",
      "\n",
      "Default (0.5) accuracy:  0.957819\n",
      "Best threshold: 0.520 → accuracy: 0.957830\n",
      "\n",
      "Top 5 thresholds:\n",
      "  0.530 → 0.957830\n",
      "  0.520 → 0.957830\n",
      "  0.500 → 0.957819\n",
      "  0.525 → 0.957819\n",
      "  0.505 → 0.957819\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Threshold Optimization\n",
    "#\n",
    "# The default threshold is 0.5. With mild class imbalance (54.7/45.3),\n",
    "# a slightly different threshold may improve accuracy.\n",
    "\n",
    "# Use whichever OOF probabilities performed better: meta-learner or weighted average\n",
    "use_meta = meta_acc >= best_wavg_acc\n",
    "if use_meta:\n",
    "    oof_final = oof_meta_probs\n",
    "    print('Using meta-learner (logistic regression) probabilities')\n",
    "else:\n",
    "    oof_final = best_weights[0] * oof_lgb + best_weights[1] * oof_xgb + best_weights[2] * oof_cb\n",
    "    print(f'Using weighted average probabilities ({best_weights})')\n",
    "\n",
    "# Sweep thresholds\n",
    "thresholds = np.arange(0.40, 0.61, 0.005)\n",
    "threshold_accs = []\n",
    "for t in thresholds:\n",
    "    acc = accuracy_score(y_train, (oof_final > t).astype(int))\n",
    "    threshold_accs.append(acc)\n",
    "\n",
    "best_threshold_idx = np.argmax(threshold_accs)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_threshold_acc = threshold_accs[best_threshold_idx]\n",
    "\n",
    "print(f'\\nDefault (0.5) accuracy:  {accuracy_score(y_train, (oof_final > 0.5).astype(int)):.6f}')\n",
    "print(f'Best threshold: {best_threshold:.3f} → accuracy: {best_threshold_acc:.6f}')\n",
    "\n",
    "# Show top thresholds\n",
    "sorted_idx = np.argsort(threshold_accs)[::-1][:5]\n",
    "print('\\nTop 5 thresholds:')\n",
    "for i in sorted_idx:\n",
    "    print(f'  {thresholds[i]:.3f} → {threshold_accs[i]:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Seed 42 ---\n",
      "  Seed 42 OOF accuracy: 0.957830\n",
      "\n",
      "--- Seed 123 ---\n",
      "  Seed 123 OOF accuracy: 0.957554\n",
      "\n",
      "--- Seed 456 ---\n",
      "  Seed 456 OOF accuracy: 0.957544\n",
      "\n",
      "--- Seed 789 ---\n",
      "  Seed 789 OOF accuracy: 0.957554\n",
      "\n",
      "--- Seed 2024 ---\n",
      "  Seed 2024 OOF accuracy: 0.957130\n",
      "\n",
      "Seed averaging complete. 5 seeds averaged.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Seed Averaging (Variance Reduction)\n",
    "#\n",
    "# Re-train the full pipeline with multiple seeds and average test predictions.\n",
    "\n",
    "seeds = [42, 123, 456, 789, 2024]\n",
    "all_test_probs = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'\\n--- Seed {seed} ---')\n",
    "    \n",
    "    # Update seeds in params (GPU settings are inherited from best_*_params)\n",
    "    lgb_params_s = {**best_lgb_params, 'random_state': seed}\n",
    "    xgb_params_s = {**best_xgb_params, 'random_state': seed}\n",
    "    cb_params_s = {**best_cb_params, 'random_seed': seed}\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # LGB (CPU — no CUDA support in this build)\n",
    "    oof_l = np.zeros(len(X_train))\n",
    "    test_l = np.zeros(len(X_test))\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        m = lgb.LGBMClassifier(**lgb_params_s)\n",
    "        m.fit(X_train[train_idx], y_train[train_idx],\n",
    "              eval_set=[(X_train[val_idx], y_train[val_idx])],\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)])\n",
    "        oof_l[val_idx] = m.predict_proba(X_train[val_idx])[:, 1]\n",
    "        test_l += m.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "    # XGB (GPU when USE_GPU=True)\n",
    "    oof_x = np.zeros(len(X_train))\n",
    "    test_x = np.zeros(len(X_test))\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        m = xgb.XGBClassifier(**xgb_params_s)\n",
    "        m.fit(X_train[train_idx], y_train[train_idx],\n",
    "              eval_set=[(X_train[val_idx], y_train[val_idx])],\n",
    "              verbose=False)\n",
    "        oof_x[val_idx] = m.predict_proba(X_train[val_idx])[:, 1]\n",
    "        test_x += m.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "    # CatBoost (GPU when USE_GPU=True)\n",
    "    oof_c = np.zeros(len(X_train))\n",
    "    test_c = np.zeros(len(X_test))\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        m = CatBoostClassifier(**cb_params_s)\n",
    "        m.fit(X_train[train_idx], y_train[train_idx],\n",
    "              eval_set=(X_train[val_idx], y_train[val_idx]),\n",
    "              verbose=0)\n",
    "        oof_c[val_idx] = m.predict_proba(X_train[val_idx])[:, 1]\n",
    "        test_c += m.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "    \n",
    "    # Combine this seed's predictions using the method chosen in Cell 10\n",
    "    if use_meta:\n",
    "        # Re-fit meta-learner on this seed's OOF\n",
    "        oof_s = np.column_stack([oof_l, oof_x, oof_c])\n",
    "        test_s = np.column_stack([test_l, test_x, test_c])\n",
    "        meta_s = LogisticRegression(C=1.0, random_state=seed, max_iter=1000)\n",
    "        meta_s.fit(oof_s, y_train)\n",
    "        seed_test_probs = meta_s.predict_proba(test_s)[:, 1]\n",
    "        seed_oof_acc = accuracy_score(y_train, meta_s.predict(oof_s))\n",
    "    else:\n",
    "        seed_test_probs = best_weights[0] * test_l + best_weights[1] * test_x + best_weights[2] * test_c\n",
    "        oof_avg = best_weights[0] * oof_l + best_weights[1] * oof_x + best_weights[2] * oof_c\n",
    "        seed_oof_acc = accuracy_score(y_train, (oof_avg > best_threshold).astype(int))\n",
    "    \n",
    "    print(f'  Seed {seed} OOF accuracy: {seed_oof_acc:.6f}')\n",
    "    all_test_probs.append(seed_test_probs)\n",
    "\n",
    "# Average across all seeds\n",
    "final_test_probs = np.mean(all_test_probs, axis=0)\n",
    "print(f'\\nSeed averaging complete. {len(seeds)} seeds averaged.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved: submission_2.csv\n",
      "Shape: (35602, 2)\n",
      "Prediction distribution:\n",
      "Overall_Experience\n",
      "1    19117\n",
      "0    16485\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Threshold used: 0.520\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Overall_Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99900001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99900002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99900003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99900004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99900005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Overall_Experience\n",
       "0  99900001                   1\n",
       "1  99900002                   1\n",
       "2  99900003                   1\n",
       "3  99900004                   0\n",
       "4  99900005                   1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 12: Generate Submission\n",
    "\n",
    "final_preds = (final_test_probs > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Overall_Experience': final_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(f'{DATA_DIR}/submission_2.csv', index=False)\n",
    "\n",
    "print(f'Submission saved: submission_2.csv')\n",
    "print(f'Shape: {submission.shape}')\n",
    "print(f'Prediction distribution:\\n{submission[\"Overall_Experience\"].value_counts()}')\n",
    "print(f'\\nThreshold used: {best_threshold:.3f}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
